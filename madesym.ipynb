{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "madesym.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "THVFVGAyUebe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lPZOMEHp3ZQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "DTYPE=tf.float32\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import Progbar\n",
        "tf.keras.backend.set_floatx('float32')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNJnhjtMGuOU"
      },
      "source": [
        "#Neural network layer with autoregressive matrix\n",
        "#I suggest to read the documentation on tfb.AutoregressiveNetwork\n",
        "class NN(Layer):\n",
        "  \n",
        "  def __init__(self, input_dim,name = 'NN'):\n",
        "    super(NN,self).__init__(name = name)\n",
        "    self.layer=tfb.AutoregressiveNetwork(params=5,activation='relu',event_shape=input_dim, hidden_units=[2048])\n",
        "    \n",
        "  def call(self,x):\n",
        "    input = x\n",
        "    x = self.layer(x)\n",
        "    #here we basically assume two normal distributions (see pdf attached)\n",
        "    #you can have many gaussians if you want but in my experience it makes things worse\n",
        "    #alpha is the mixing parameter between two gaussians \n",
        "    #for more gaussians The layer output (now given by x) can be feeded into \n",
        "    #the softmax layer and layers for parameters. (i.e.) params=2*Ng+(Ng) \n",
        "    #where 2Ng - log_scale and means of Ng gaussians\n",
        "    #Ng is a set of mixing parameters to feed into the softmax function\n",
        "    loc_1,loc_2,scale_1,scale_2,alpha = tf.split(x,5,-1)\n",
        "    #4 is to cover the presumable range of x mean is set to .4 just so the gaussians arent on top of each other\n",
        "    loc_1 = 4*tf.nn.tanh(loc_1)+0.4\n",
        "    loc_2 = 4*tf.nn.tanh(loc_2)-0.4\n",
        "    scale_1 = tf.nn.sigmoid(scale_1)\n",
        "    scale_2 = tf.nn.sigmoid(scale_2)\n",
        "    alpha = tf.nn.sigmoid(alpha)\n",
        "    out = tf.concat([loc_1,loc_2,scale_1,scale_2,alpha],axis=-1)\n",
        "    return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN5iblaMUsEZ"
      },
      "source": [
        "input_dim=100\n",
        "input_shape=(input_dim,)\n",
        "inputs = keras.Input(shape=input_shape)\n",
        "#a very simple autoregresive model (5 parameters from 1 layer)\n",
        "x = tf.keras.layers.experimental.preprocessing.Rescaling(.5)(inputs)\n",
        "out = NN(input_dim)(x) \n",
        "MADE_V = keras.Model(inputs, out)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCyZZrvL7XH8"
      },
      "source": [
        "#Note that it is very slow"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EILtDMM92fn"
      },
      "source": [
        "#basically the same energy as before\n",
        "@tf.function \n",
        "def Energy(y):\n",
        "        lamd = 1.\n",
        "        f = np.sqrt(2.)\n",
        "        a = 0.1\n",
        "        m = 0.5\n",
        "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
        "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
        "        return kinetic_energy + a*potential_energy\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PDm7ZO0tbuP"
      },
      "source": [
        "input_size=100\n",
        "#Sampling configurations\n",
        "def NN_Sample(sample_size):\n",
        "    configs = np.zeros((sample_size,input_size),dtype=np.float32)\n",
        "    params = np.zeros((sample_size,input_size,5),dtype=np.float32)\n",
        "    for cell in np.arange(input_size):\n",
        "        params[:,cell,:] = MADE_V(configs)[:,cell,:]\n",
        "        shifts_1,shifts_2, scales_1,scales_2,mix = tf.unstack(params[:,cell,:],5,axis=-1)\n",
        "        bimix_gauss2 = tfd.Mixture(\n",
        "                        cat=tfd.Categorical(probs=tf.stack([mix,1.-mix],axis=-1)),\n",
        "                        components=[\n",
        "                          tfd.Normal(loc=shifts_1, scale=scales_1),\n",
        "                          tfd.Normal(loc=shifts_2, scale=scales_2)])\n",
        "        configs[:,cell]=bimix_gauss2.sample()\n",
        "    return configs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLwqMAd7okO4",
        "outputId": "27612ca7-ae88-4014-a699-758bedbede57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "@tf.function\n",
        "def log_probability(xs,output):\n",
        "    shift_1,shift_2,scale_1,scale_2,mix = tf.unstack(output,5,axis=-1)\n",
        "    bimix_gauss = tfd.Mixture(\n",
        "                        cat=tfd.Categorical(probs=tf.stack([mix,1-mix],axis=-1)),\n",
        "                        components=[\n",
        "                          tfd.Normal(loc=shift_1, scale=scale_1),\n",
        "                          tfd.Normal(loc=shift_2, scale=scale_2)])\n",
        "    pdf = bimix_gauss.log_prob(xs) \n",
        "    return tf.reduce_sum(pdf,axis=-1)\n",
        "@tf.function\n",
        "def KL_Divergence(xs,energy):\n",
        "    out = MADE_V(xs)\n",
        "    log_p = log_probability(xs,out)\n",
        "    KL_D = log_p+energy\n",
        "    return tf.math.reduce_mean(KL_D)\n",
        "@tf.function\n",
        "def KL_loss(xs,energy,KL_D):\n",
        "    out = MADE_V(xs)\n",
        "    log_p = log_probability(xs,out)\n",
        "    t = log_p*(energy+0.5*log_p)/tf.abs(KL_D)\n",
        "    return tf.math.reduce_mean(t)\n",
        "\n",
        "lr_decay = .1\n",
        "learning_rate = 1e-5\n",
        "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "checkpoint_directory = '/drive/My Drive/saved_models_MADE_21/'\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=MADE_V)\n",
        "status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 20\n",
        "n_iter = 1000\n",
        "n_samples = 1024\n",
        "for epoch in range(n_epochs):\n",
        "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
        "    progbar = Progbar(n_iter)\n",
        "    \n",
        "    \n",
        "    losses = []\n",
        "    for iter in range(n_iter):\n",
        "        \n",
        "        xs_m = NN_Sample(n_samples)\n",
        "        es_m = Energy(xs_m)\n",
        "        KL_D = KL_Divergence(xs_m,es_m)\n",
        "        with tf.GradientTape() as ae_tape:\n",
        "            loss = KL_loss(xs_m,es_m,KL_D)\n",
        "        \n",
        "        gradients = ae_tape.gradient(loss, MADE_V.trainable_variables)\n",
        "\n",
        "        \n",
        "        losses.append(KL_D)\n",
        "        progbar.add(1, values=[('loss', KL_D)])\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients,MADE_V.trainable_variables))\n",
        "    print('  Mean loss: {}'.format(np.mean(losses)))\n",
        "    print('  Var of loss: {}'.format(np.var(losses)))\n",
        "    checkpoint.save(checkpoint_directory+'ckpt_{}'.format(epoch))    \n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/20\n",
            "  71/1000 [=>............................] - ETA: 42:37 - loss: 142.6220"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tU7zmSXIp7dW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}